{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Natural Language Processing Project-2     PART-1\n",
    " by ARYAN JAIN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARY\n",
    "\n",
    "<b>Context:</b>\n",
    "    The objective of this project is to build a text classification model that analyses the customer's sentiments \n",
    "    based on their reviews in the IMDB database. \n",
    "    The model uses a complex deep learning model to build an embedding layer followed by a classification algorithm to\n",
    "    analyse the sentiment of the customers.\n",
    "    \n",
    "    \n",
    "<b>Data Description:</b>    \n",
    "    The Dataset of 50,000 movie reviews from IMDB, labelled by sentiment (positive/negative). \n",
    "    Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
    "    For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the \n",
    "    most frequent word. \n",
    "    Use the first 20 words from each review to speed up training, using a max vocabulary size of 10,000. \n",
    "    As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "    \n",
    "\n",
    "<b>Domain:</b>\n",
    " Digital content and entertainment industry\n",
    "    \n",
    "    \n",
    "<b>Objectives:</b>\n",
    "    Build a sequential NLP classifier which can use input text parameters to determine the customer sentiments.\n",
    " \n",
    "<b>Key Tasks:</b>\n",
    "\n",
    "     - Import and analyse the data set (imbd, 1000 most frequent words) \n",
    "     - Perform relevant sequence adding on the data\n",
    "     - Perform data analysis: Print shape of features and labels, Print value of any one feature and it's label\n",
    "     - Decode the feature value to get original sentence\n",
    "     - Design, train, tune and test a sequential model\n",
    "     - Use the designed model to print the prediction on any one sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rCk5oaTTOdGW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the random number generator\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importing the in-built IMDB dataset\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data (imdb dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yUh-pblaRFm",
    "outputId": "bc007454-d7a4-4ac5-b952-438c78466a97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most frequent 1000 words\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxperGzlU1oG"
   },
   "source": [
    "### Perform relevant sequence adding on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence adding on data \n",
    "\n",
    "    # First we have to decide a fixed length for each review\n",
    "    # Then we need to either trim or pad every review to that size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtPTE4Tnr2K2",
    "outputId": "875b3389-f321-49ea-aa47-9036dcc0dbe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's investigate length of the first review\n",
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the length of all the Reviews in a list and then use it further to find the number of average words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "srPV7CchWP7w"
   },
   "outputs": [],
   "source": [
    "# concatenate both X_train and X_test so that we can get the full set of 50,000 records\n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "PIjhVAO7W88p",
    "outputId": "e0466bea-1bca-48d6-eba9-ab32783ba219"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_review_lengths = [len(x) for x in X]\n",
    "plt.boxplot(list_review_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7q0DUyeXhiX",
    "outputId": "8363ad5d-373c-41fc-96ee-55e2c204e9ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = %f 234.75892\n",
      "Std = %f 172.91149458735703\n",
      "Min = %f 7\n",
      "Max = %f 2494\n"
     ]
    }
   ],
   "source": [
    "# Let's print and review review lengths\n",
    "\n",
    "print (\"Mean = %f\" , np.mean(list_review_lengths))\n",
    "print (\"Std = %f\" , np.std(list_review_lengths))\n",
    "print (\"Min = %f\" , np.min(list_review_lengths))\n",
    "print (\"Max = %f\" , np.max(list_review_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean review length = 234 \n",
    "# Standard deviation = 172. \n",
    "\n",
    "# I will use total length = 450."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "V2cUwdX5r2K5"
   },
   "outputs": [],
   "source": [
    "# Based on above analysis, let's pad to total length of 450\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train_450 = pad_sequences(X_train, maxlen=450)\n",
    "X_test_450 = pad_sequences(X_test, maxlen=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKeb7kfQZmTY"
   },
   "source": [
    "### Print shape of features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIS2uiODaBqP",
    "outputId": "4619592c-878f-4be2-946d-a851d31598a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 450), (25000,), (25000, 450), (25000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_450.shape, y_train.shape, X_test_450.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print one feature and it's label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_YtFeNiaIOA",
    "outputId": "5adf6545-9907-4aef-b446-9b59e0c1cfda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    1,  591,  202,\n",
       "         14,   31,    6,  717,   10,   10,    2,    2,    5,    4,  360,\n",
       "          7,    4,  177, 5760,  394,  354,    4,  123,    9, 1035, 1035,\n",
       "       1035,   10,   10,   13,   92,  124,   89,  488, 7944,  100,   28,\n",
       "       1668,   14,   31,   23,   27, 7479,   29,  220,  468,    8,  124,\n",
       "         14,  286,  170,    8,  157,   46,    5,   27,  239,   16,  179,\n",
       "          2,   38,   32,   25, 7944,  451,  202,   14,    6,  717],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_450[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKXavXWLa6iM",
    "outputId": "03e4bd6c-4aa4-485b-b06f-898b4650aa69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_MYYx0rddo9",
    "outputId": "ee9ff88e-6abe-4782-f5e2-e5a2dd55332a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    1,  261,\n",
       "         13,   69,  110,    2,   11,    6,  750,   96,  145,   11,    2,\n",
       "         13,  426,  377,  233,    7,    4,  114,  549,   18, 3482, 1218,\n",
       "          7, 3199, 2102,  620,    5,  997,  429,    6, 4157,    7, 1654,\n",
       "       3404,    5, 1387, 2795,    4,  277,   10,   10,  103,  886,   49,\n",
       "          7,    4,   85,  857,   13, 1637,   56,    6, 1039,    7,    4,\n",
       "       4700,  626,  288,    8,  280,  174, 2541,    4,  182,    7,    2,\n",
       "         10,   10,   12,  505,   46,   14,    9,   31,    7,  148,  108,\n",
       "       1055,  315,    4, 7423,   15,   62,  140, 2555,    8,  374,  639,\n",
       "          4,   22,  381, 5810,    2, 3199, 2102,   17, 4409,    2,    2,\n",
       "         46,    7,    4, 1336,    8,    2,    4, 2952,    7,    2,    8,\n",
       "       2541,    5, 2363, 1176,    4,  500,    6,    2,    2,    2,   34,\n",
       "          4, 4698,   37, 9090,   27,   84,   34, 4578,   51,  934,   40,\n",
       "          2,  671,    4, 3359,    7,    4, 1973, 2290,    4,  323, 1650,\n",
       "       4824, 1510, 4409,    9, 4415,   11,    4, 1647, 1733,   34, 2549,\n",
       "          2,   37,  115, 2466,   42,  889,    4,  313,  280, 4415,  497,\n",
       "          8, 3890,   11,   19,    4, 5751,   34, 1658,    6, 1927,  767,\n",
       "          2,   19,    2,    7, 1336,    5,  428, 2964,    8,  135,    2,\n",
       "        659,  309,  620,    5,  997,   18,   27,  113,  367,    4, 1654,\n",
       "       1986,    7,    2,   23,   27,   96,    8,    6, 1304,   19, 4205,\n",
       "          5,    4,  500,   10,   10,  323, 3199, 2102,   16, 1815, 3009,\n",
       "         34,  167,  670,    2,   37,   12,  272,   40,   16,  115,   23,\n",
       "          4,  270,    4,   38,  446,  229,    9,   43,   14,  499,    7,\n",
       "       4767,   67,   85,  857,   18,    4,  111,    2,  469,    4,  513,\n",
       "          7,    2,   47,    6,  171,   52,  388,   21,    9, 1116, 2262,\n",
       "         34,   78,  802,    4,  277,    2,  131,  225,    4,    2,    7,\n",
       "          6,   52,  206, 1154,  133,    6, 2579, 1991,  310,   19,   53,\n",
       "       8904,  206,    5, 4291, 1062,  238,   60,   30,  184,   52],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_450[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLoCgs9aj4sp",
    "outputId": "31e7d2d9-731c-47d4-9abc-99d4d6c8122d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the feature value to derive orignal sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GVnmmpeiiCFx"
   },
   "outputs": [],
   "source": [
    "# Retrieve the word index file mapping words to indices\n",
    "word_index = imdb.get_word_index()\n",
    "# Reverse the word index to obtain a dict mapping indices to words\n",
    "inverted_word_index = dict([(i, word) for (word, i) in word_index.items()])\n",
    "# As per information on the web, indices are off by 3 because 0, 1, and 2 are reserved indices for \"padding\", \n",
    "# \"Start of sequence\" and \"unknown\"\n",
    "# So, we need to take care of that while decoding.\n",
    "# Decode the first sequence in the dataset\n",
    "decoded_sequence = \" \".join([inverted_word_index.get(i-3, '0') for i in X[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tpa5GFAzieaY",
    "outputId": "ad2167e8-511a-4ede-df19-5d85f61a41f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert 0 is an amazing actor and now the same being director 0 father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for 0 and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also 0 to the two little boy's that played the 0 of norman and paul they were just brilliant children are often left out of the 0 list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytE7skhnZdLR",
    "outputId": "f8d9f16a-4a88-468a-ff98-8bc34abeed31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0] # 1 means the review is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAekzkZVjRfJ",
    "outputId": "ef84a491-bd1b-4724-bbe0-3e39134721e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 although i had seen 0 in a theater way back in 0 i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of 0 br br it turns out this is one of those films produced during the '80s that would go directly to video today the film stars champion 0 kurt thomas as jonathan 0 0 out of the blue to 0 the nation of 0 to enter and hopefully win the game a 0 0 0 by the khan who encourages his people by yelling what sounds like 0 power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess 0 who never speaks or leaves the house once trained tries to blend in with the locals by wearing a bright red 0 with 0 of blue and white needless to say 0 finds himself running and fighting for his life along the stone streets of 0 on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert 0 who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many 0 throughout the town of 0 has a few good moments but is ultimately ruined by bad editing the ending 0 still there's the 0 of a good action adventure here a hong kong version with more visceral action and faster pace might even be pretty good\n"
     ]
    }
   ],
   "source": [
    "decoded_sequence = \" \".join([inverted_word_index.get(i-3, \"0\") for i in X[1000]])\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuxVdRiIZr7W",
    "outputId": "eafd4aba-6d9e-4fee-983c-992ce634d82d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1000] # 0 means the review is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now its time to design, train and tune our sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "qemRxPUEkvPg"
   },
   "outputs": [],
   "source": [
    "# The problem statement recommends using first 20 words of the review, so I will set the max length to 20.\n",
    "\n",
    "X_train_20 = pad_sequences(X_train, maxlen=20) \n",
    "X_test_20 = pad_sequences(X_test, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2JN8Ro6KbVe",
    "outputId": "13c0ef9c-d1de-451b-d11f-05f2902c8660"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 20), (25000,), (25000, 20), (25000,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_20.shape, y_train.shape, X_test_20.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "buT-b654r2K8",
    "outputId": "5c8f69ed-9dc7-40dd-943b-f7ac12950f40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 32)            320000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the sequential model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32, input_length=20)) # use embedding vector size of 32 dimesions\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100)) # 100 LSTM units\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VA_Bkj4er2K_",
    "outputId": "ee3430a4-fc70-4597-9fab-a9417e4b8c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 20) (22500,)\n",
      "(2500, 20) (2500,)\n"
     ]
    }
   ],
   "source": [
    "# Splt dataset for testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "x_val_20, x_test_20, y_val_20, y_test_20 = train_test_split(X_test_20,y_test, test_size = 0.1, random_state = 7)\n",
    "print(x_val_20.shape,y_val_20.shape)\n",
    "print(x_test_20.shape,y_test_20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0jehV74oNNK",
    "outputId": "849b6257-aafa-4941-a21a-023e0e745119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 - 15s - loss: 0.5355 - accuracy: 0.7188 - val_loss: 0.4747 - val_accuracy: 0.7696\n",
      "Epoch 2/20\n",
      "782/782 - 13s - loss: 0.4166 - accuracy: 0.8082 - val_loss: 0.4749 - val_accuracy: 0.7693\n",
      "Epoch 3/20\n",
      "782/782 - 12s - loss: 0.3648 - accuracy: 0.8364 - val_loss: 0.5208 - val_accuracy: 0.7592\n",
      "Epoch 4/20\n",
      "782/782 - 13s - loss: 0.3153 - accuracy: 0.8632 - val_loss: 0.5309 - val_accuracy: 0.7628\n",
      "Epoch 5/20\n",
      "782/782 - 16s - loss: 0.2667 - accuracy: 0.8863 - val_loss: 0.6585 - val_accuracy: 0.7490\n",
      "Epoch 6/20\n",
      "782/782 - 16s - loss: 0.2232 - accuracy: 0.9077 - val_loss: 0.6744 - val_accuracy: 0.7476\n",
      "Epoch 7/20\n",
      "782/782 - 14s - loss: 0.1805 - accuracy: 0.9268 - val_loss: 0.7626 - val_accuracy: 0.7432\n",
      "Epoch 8/20\n",
      "782/782 - 13s - loss: 0.1519 - accuracy: 0.9412 - val_loss: 0.8330 - val_accuracy: 0.7476\n",
      "Epoch 9/20\n",
      "782/782 - 13s - loss: 0.1272 - accuracy: 0.9521 - val_loss: 0.9595 - val_accuracy: 0.7383\n",
      "Epoch 10/20\n",
      "782/782 - 14s - loss: 0.1062 - accuracy: 0.9578 - val_loss: 0.9346 - val_accuracy: 0.7392\n",
      "Epoch 11/20\n",
      "782/782 - 13s - loss: 0.0875 - accuracy: 0.9670 - val_loss: 1.2201 - val_accuracy: 0.7343\n",
      "Epoch 12/20\n",
      "782/782 - 12s - loss: 0.0802 - accuracy: 0.9702 - val_loss: 1.1675 - val_accuracy: 0.7346\n",
      "Epoch 13/20\n",
      "782/782 - 14s - loss: 0.0673 - accuracy: 0.9755 - val_loss: 1.2954 - val_accuracy: 0.7285\n",
      "Epoch 14/20\n",
      "782/782 - 13s - loss: 0.0589 - accuracy: 0.9787 - val_loss: 1.2858 - val_accuracy: 0.7348\n",
      "Epoch 15/20\n",
      "782/782 - 16s - loss: 0.0526 - accuracy: 0.9818 - val_loss: 1.3847 - val_accuracy: 0.7311\n",
      "Epoch 16/20\n",
      "782/782 - 15s - loss: 0.0449 - accuracy: 0.9840 - val_loss: 1.4850 - val_accuracy: 0.7333\n",
      "Epoch 17/20\n",
      "782/782 - 13s - loss: 0.0427 - accuracy: 0.9840 - val_loss: 1.5446 - val_accuracy: 0.7303\n",
      "Epoch 18/20\n",
      "782/782 - 12s - loss: 0.0418 - accuracy: 0.9852 - val_loss: 1.5682 - val_accuracy: 0.7286\n",
      "Epoch 19/20\n",
      "782/782 - 15s - loss: 0.0357 - accuracy: 0.9866 - val_loss: 1.4943 - val_accuracy: 0.7294\n",
      "Epoch 20/20\n",
      "782/782 - 13s - loss: 0.0372 - accuracy: 0.9866 - val_loss: 1.6164 - val_accuracy: 0.7288\n",
      "Accuracy: 72.87999987602234\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_20, y_train, validation_data=(x_val_20, y_val_20), epochs=20, batch_size=32, verbose=2) \n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_val_20, y_val_20, verbose=0)\n",
    "\n",
    "# print performance \n",
    "print(f\"Accuracy: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the model to predict few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geZ2QJq_vKpD",
    "outputId": "f571e4ed-e4ca-4768-99e0-e074013c470b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 20) for input KerasTensor(type_spec=TensorSpec(shape=(None, 20), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "Positive : 0.06190726161003113\n",
      "Negative : 0.9380927383899689\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "\n",
    "pred = model.predict(x_test_20[10])[0][0]\n",
    "print(f\"Positive : {pred}\")\n",
    "print(f\"Negative : {1-pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 french horror cinema has seen something of a revival over the last couple of years with great films such as inside and 0 romance 0 on to the scene 0 0 the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made 0 was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is 0 by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named 0 sent to prison for fraud he is put in a cell with three others the quietly insane 0 body building 0 marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old 0 after 0 part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that 0 makes the best of it's 0 as despite it's 0 the film never actually feels restrained and manages to flow well throughout director eric 0 provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell 0 that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really 0 people and this film proves that as the director 0 that we can never really be sure of exactly what is round the corner and this helps to ensure that 0 actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall 0 is a truly great horror film and one of the best of the decade highly recommended viewing\n"
     ]
    }
   ],
   "source": [
    "# Decoding original\n",
    "\n",
    "decoded_sequence = \" \".join([inverted_word_index.get(i-3, '0') for i in X_train[10]])\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_20[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: Negative, Label - Negative (seems correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RN0pczf1wcj7",
    "outputId": "0da333d7-c589-4454-fefb-9eaf385e6205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive : 0.08814433217048645\n",
      "Negative : 0.9118556678295135\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test_20[100])[0][0]\n",
    "print(f\"Positive : {pred}\")\n",
    "print(f\"Negative : {1-pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of 0 drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was 0 on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep 0 the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the 0 box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\n"
     ]
    }
   ],
   "source": [
    "# Decoding original\n",
    "\n",
    "decoded_sequence = \" \".join([inverted_word_index.get(i-3, '0') for i in X_train[100]])\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjzkhGPFwi4a",
    "outputId": "e47035b4-76fd-4941-a099-273ef494e9a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_20[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: Negative, Label - Negative (seems correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive : 0.5456298589706421\n",
      "Negative : 0.4543701410293579\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test_20[400])[0][0]\n",
    "print(f\"Positive : {pred}\")\n",
    "print(f\"Negative : {1-pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 in cold blood has to be 0 as first rate movie making even if the subject matter is about as grim as it gets in the world of make believe but film noir fans should definitely find this one a gripping piece of work based as it is on a true life crime spree br br it opens with 0 jones' music under the credits and 0 dramatic views of a highway bus heading toward kansas city effectively setting the mood of the film even before the credits end the b w photography of conrad hall does a superb job right from the start br br also clear from the start robert blake and scott wilson are natural born actors they do a great job of portraying free spirited buddies looking for the next thrill ever see a millionaire 0 in the electric hair hell no there are two kinds of rules in this world one for the rich and one for the poor says wilson taking a 0 of alcohol behind the wheel br br both are destined to cross the path of a farm family showing no mercy and leaving no witnesses behind br br blake 0 about movies and thinking of hunting for gold in mexico says remember bogart in 0 of the 0 0 an ironic moment because blake himself was in the film as a little boy selling 0 tickets i got you 0 for a natural born killer wilson tells blake br br john forsythe is one of the lead detectives on the case discovering that all four family members were tied up shot in the head and one had his throat cut don't people around here lock doors asks paul stewart they will tonight is the 0 0 br br after the murders the killers discover that there was no big fat safe in the wall like their prison 0 told them so in the end it was truly a stupid senseless crime the question is why did they do it and this is something the second half of the film explores in depth it takes an hour and a half into the movie before the detectives catch up with the killers and begin the 0 br br it's these final scenes that carry the most conviction and the most interest as the boys are told they've made numerous mistakes and left a living witness the actual events up to and including the murder are saved until the end it makes no sense blake tells forsythe mr cutter was a very nice gentleman i thought so right up until the time i cut his throat the screenplay by richard brooks is 0 and to the point and so is his direction br br 0 up brilliant depiction of two 0 young men on a crime spree that made no sense then or now for a mere 0 chilling\n"
     ]
    }
   ],
   "source": [
    "# Decoding original\n",
    "\n",
    "decoded_sequence = \" \".join([inverted_word_index.get(i-3, '0') for i in X_train[400]])\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_20[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: Negative, Label - Positive (that seems incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TR1DYuYyabwy",
    "outputId": "f9988ea4-3f24-4570-bc76-eeb5d327a297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 20) for input KerasTensor(type_spec=TensorSpec(shape=(None, 20), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (32, 1).\n",
      "Positive : 0.9998973608016968\n",
      "Negative : 0.00010263919830322266\n"
     ]
    }
   ],
   "source": [
    "# Let's predict and validate based on decoded original\n",
    "\n",
    "pred = model.predict(X_train[200])[0][0]\n",
    "print(f\"Positive : {pred}\")\n",
    "print(f\"Negative : {1-pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-VZANtD1IyS",
    "outputId": "dc632146-aa77-4bb9-9ffe-e3d6f324fdf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 this is a bit long 2 hours 20 minutes but it had a a lot of the famous pearl buck novel in it in other words a lot of ground to cover br br it was soap 0 at times but had some visually dramatic moments too 0 off by a 0 attack at the end of the film that was astounding to view considering this film is about 70 years old the special effects crew on this film did a spectacular job br br paul muni and 0 rainer were award winning actors in their day and they don't disappoint here both giving powerful performances the only problem is credibility as all the 0 are played by 0 and some of them like walter connolly just don't look real i'd like to see a re make of this movie with all asian actors not for pc reasons but to simply make the story look and sound more credible\n"
     ]
    }
   ],
   "source": [
    "# Decoding original\n",
    "\n",
    "decoded_sequence = \" \".join([inverted_word_index.get(i-3, '0') for i in X_train[200]])\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0leyc2O1ggb",
    "outputId": "6ab1d972-03ca-4b55-a74b-316e1ce54a43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check label\n",
    "y_train[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction (negative) appears corerct based on the decoded sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF PART 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP2_Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
