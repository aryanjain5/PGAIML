{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Computer Vision #2\n",
    " \n",
    "                                 by ARYAN JAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARY\n",
    "\n",
    "<b>Context:</b>\n",
    "    Company X owns a movie application and repository which caters movie streaming to millions of users who on \n",
    "    subscription basis.Company wants to automate the process of cast and crew information in each scene from a \n",
    "    movie such that when a user pauses on the movie and clicks on cast information button, the app will show \n",
    "    details of the actor in the scene. Company has an in-house computer vision and multimedia experts who need to detect faces from screen shots \n",
    "    from the movie scene.\n",
    "    \n",
    "    \n",
    "<b>Data Description:</b>    \n",
    "    The dataset comprises of images and its mask where there is a human face\n",
    "    \n",
    "    File name: Part1-Traindata-images.npy\n",
    "    \n",
    "<b>Domain:</b>\n",
    " Entertainment\n",
    "    \n",
    "    \n",
    "<b>Objectives:</b>\n",
    "    Face detection from training images\n",
    " \n",
    "<b>Key Tasks:</b>\n",
    "\n",
    "    - Import the data\n",
    "    - Create features (images) and labels (mask) using that data\n",
    "    - Design a face mask detection model, using U-net along with pre-trained transfer learning models\n",
    "    - Design own Dice Coefficient and Loss function. Train, tune and test the model\n",
    "    - Evaluate the model using testing data\n",
    "    - Use the “Prediction image” as an input to your designed model and display the output of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Deep Learning\n",
    "import cv2\n",
    "from skimage import io,transform\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.layers import Concatenate, UpSampling2D, Conv2D, Reshape, Activation, BatchNormalization, SpatialDropout2D\n",
    "from tensorflow.python.keras.preprocessing.image import image, load_img, ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.backend import log, epsilon\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Python Imaging Library; for opening, manipulating, and saving many different image file formats#\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file name: Part1-Traindata-images.npy\n",
    "# NPY file means file created by NumPy python library\n",
    "# It contains an array saved in the NumPy file format. \n",
    "# These NPY files contain the data needed to recontruct the data, like the datatypes and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to keep allow_pickle = True otherwise it thows an error\n",
    "data = np.load('Part 1- Train data - images.npy', allow_pickle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the data size\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data file has 409 rows, 2 columns\n",
    "# Based on project description, it seems the file contains 409 images & corresponding detail\n",
    "\n",
    "# I will investigate to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first column\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(data[7][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first column contains image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 2nd column\n",
    "\n",
    "data[10][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 2nd column is bouding box coordinates, including the label, around the human face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display few more images to confirm\n",
    "\n",
    "fig = plt.figure(figsize = (15, 7.2))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(data[98][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[147][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[252][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 2nd column for one of the above images, expecting it to be bounding box co-ordinates\n",
    "\n",
    "data[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features & labels\n",
    "\n",
    "# Based on the project description and understanding the training dataset:\n",
    "        # the feature means images\n",
    "        # labels means face mask (identified using box coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate numpy arrays for facemasks and images with zero\n",
    "# Use the array size = number of images in the training data set\n",
    "\n",
    "# The intent is to use MobileNet for the pre-trained layers:\n",
    "        # Input size should be 224x224 for masks\n",
    "        # input size should be 224x224x3 for images, last dimension is for channels\n",
    "        # We will also have to resize images to 224x224\n",
    "\n",
    "# library used:\n",
    "    # from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "# Instantiating numpy arrays\n",
    "masks  = np.zeros((int(data.shape[0]), 224, 224))              #array of size 224x224, initialized with ZEROs\n",
    "images = np.zeros((int(data.shape[0]), 224, 224, 3))           #array of size 224x224x3, initialized with ZEROs\n",
    "\n",
    "# Resizing images to 224x224\n",
    "for i in range(data.shape[0]):\n",
    "    img = data[i][0]\n",
    "    img = cv2.resize(img, dsize = (224, 224), interpolation = cv2.INTER_CUBIC)\n",
    "    try:\n",
    "        img = img[:, :, :3]\n",
    "    except:\n",
    "        continue\n",
    "    images[i] = preprocess_input(np.array(img, dtype = np.float32)) # pre-process the input as needed by Mobile Net\n",
    "    \n",
    "    \n",
    "    for index in data[i][1]:\n",
    "        # get the bounding box co-ordinates and create the mask\n",
    "        x1 = int(index['points'][0]['x'] * 224)\n",
    "        x2 = int(index['points'][1]['x'] * 224)\n",
    "        y1 = int(index['points'][0]['y'] * 224)\n",
    "        y2 = int(index['points'][1]['y'] * 224)\n",
    "        #Generate the face mask bounding box\n",
    "        masks[i][y1:y2, x1:x2] = 1            # remaining regions are marked as 0 when we initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at shape of the image and mask arrays created\n",
    "\n",
    "images.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have resized the images to 224x224x3, stored in a dataframe called images\n",
    "# The bounding box image based on the co-ordinates is stored in a dataframe called masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display few images from the dataframe:\n",
    "\n",
    "    # I will look at same images displayed in earlier steps, to understand impact of resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[7]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[98]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the face is yellow\n",
    "\n",
    "plt.imshow(masks[98]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the strategy:\n",
    "\n",
    "    # Define variables\n",
    "    # Splitting the data\n",
    "    # Using MobileNet with ImageNet weights, U-Net layers at the end\n",
    "    # The U-Net model using pre-trained ImageNet as backbone\n",
    "    # Upsampling in the final layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting values for variables\n",
    "random_s = 0               \n",
    "test_s = 0.2 \n",
    "\n",
    "alpha_s = 1 \n",
    "# Alpha value of 1 to get the entire MobileNet\n",
    "\n",
    "img_h  = 224   # Image height\n",
    "img_w  = 224   # Image width\n",
    "img_c = 3\n",
    "msk_h  = 224   # Mask height\n",
    "msk_w  = 224   # Mask width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size = test_s, random_state = random_s, shuffle = False)\n",
    "\n",
    "# Display training and validation data shapes\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per above steps, I will be using MobileNet as the base model\n",
    "    # Let's take a look at base MobileNet model summary:\n",
    "    \n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "\n",
    "mn = MobileNet(input_shape=(img_h, img_w, 3), include_top=True, alpha=alpha_s, weights=\"imagenet\")\n",
    "mn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: creating the Mask detection model\n",
    "\n",
    "# Libraries (already declared in the beginning of this notebook)\n",
    "\n",
    "# from tensorflow.keras.layers import Concatenate, Conv2D, Reshape, UpSampling2D, BatchNormalization\n",
    "# from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(prevlayer, filters, prefix, strides=(1, 1)):\n",
    "    conv = Conv2D(filters, (3, 3), padding = 'same', kernel_initializer = 'he_normal', strides = strides, name = prefix + '_conv')(prevlayer)\n",
    "    conv = BatchNormalization(name = prefix + 'BatchNormalization')(conv)\n",
    "    conv = Activation('relu', name = prefix + 'ActivationLayer')(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library to use, already imported in earlier step\n",
    "# from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "\n",
    "\n",
    "# Function to create model\n",
    "\n",
    "def create_model(trainable = True):\n",
    "    model = MobileNet(input_shape = (img_h, img_w, img_c), include_top = False, alpha = alpha_s, weights = 'imagenet')\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = trainable\n",
    "    \n",
    "    block1 = model.get_layer('conv_pw_13_relu').output\n",
    "    block2 = model.get_layer('conv_pw_11_relu').output\n",
    "    block3 = model.get_layer('conv_pw_5_relu').output\n",
    "    block4 = model.get_layer('conv_pw_3_relu').output\n",
    "    block5 = model.get_layer('conv_pw_1_relu').output\n",
    "    \n",
    "    up1 = Concatenate()([UpSampling2D()(block1), block2])\n",
    "    conv6 = conv_block(up1, 256, 'Conv_6_1')\n",
    "    conv6 = conv_block(conv6, 256, 'Conv_6_2')\n",
    "\n",
    "    up2 = Concatenate()([UpSampling2D()(conv6), block3])\n",
    "    conv7 = conv_block(up2, 256, 'Conv_7_1')\n",
    "    conv7 = conv_block(conv7, 256, 'Conv_7_2')\n",
    "\n",
    "    up3 = Concatenate()([UpSampling2D()(conv7), block4])\n",
    "    conv8 = conv_block(up3, 192, 'Conv_8_1')\n",
    "    conv8 = conv_block(conv8, 128, 'Conv_8_2')\n",
    "\n",
    "    up4 = Concatenate()([UpSampling2D()(conv8), block5])\n",
    "    conv9 = conv_block(up4, 96, 'Conv_9_1')\n",
    "    conv9 = conv_block(conv9, 64, 'Conv_9_2')\n",
    "\n",
    "    up5 = Concatenate()([UpSampling2D()(conv9), model.input])\n",
    "    conv10 = conv_block(up5, 48, 'Conv_10_1')\n",
    "    conv10 = conv_block(conv10, 32, 'Conv_10_2')\n",
    "    conv10 = SpatialDropout2D(0.2)(conv10)\n",
    "    \n",
    "    x = Conv2D(1, (1, 1), activation = 'sigmoid')(conv10)\n",
    "    x = Reshape((img_h, img_w))(x)\n",
    "    return Model(inputs = model.input, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the create_model function\n",
    "\n",
    "model = create_model(True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice Coefficient and Loss function; define functions to calculate:\n",
    "\n",
    "# Dice Coefficient = (2*|X ☊ Y|) / (|X| + |Y|)   # X = predicted set of pixels, Y = ground truth\n",
    "\n",
    "   # Dice Coefficient \n",
    "   # Loss using binary cross-entropy function from keras.losses and the calculated dice co-efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    num = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    den = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "    return num / (den + tf.keras.backend.epsilon()) # Adding the epsilon value to make sure we are not dividing by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - log(dice_coefficient(y_true, y_pred) + epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: Compile the model\n",
    "    # Optimizer => adam\n",
    "    # Metrics => dice coefficient\n",
    "    # Loss => binary cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and compile the model\n",
    "\n",
    "adam = Adam(lr = 1e-4, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad = False)\n",
    "model.compile(loss = loss, optimizer = adam, metrics = [dice_coefficient])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Blue'>We will use the Adam optimizer and use our defined loss function and dice_coefficient as metric</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Checkpoint \n",
    "\n",
    "# The best performing model weights will be saved (see first parameter in ModelCheckpoint)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model_{loss:.2f}.h5', monitor = 'val_loss', verbose = 1, \n",
    "                             save_best_only = True, save_weights_only = True, mode = 'min', period = 1)\n",
    "\n",
    "stop = EarlyStopping(monitor = 'val_loss', patience = 5, mode = 'min')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 5, min_lr = 1e-6, verbose = 1, mode = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit / Train the model\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 30, batch_size = 1, callbacks = [checkpoint, reduce_lr, stop], \n",
    "          validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "model.evaluate(X_val, y_val, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned above: \n",
    "     # the best performing model weights is saved in model_0.29.h5\n",
    "     # I will evaluate performance with this weight as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict the Mask for the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "filename = 'Part 1Test Data - Prediction Image.jpeg' # load the test image\n",
    "\n",
    "# plot the original test image\n",
    "p_img = cv2.imread(filename)\n",
    "plt.imshow(p_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets predict the face mask using our model .\n",
    "# Working earlier, now error\n",
    "\n",
    "test_img = cv2.resize(p_img, dsize = (img_w, img_h), interpolation = cv2.INTER_CUBIC)\n",
    "test_img = test_img[:, :, :3]\n",
    "MNet_scaled = preprocess_input(np.array(test_img, dtype=np.float32)) # apply pre-processing as needed for MobileNet\n",
    "\n",
    "# Now lets create the mask on the original test image by marking pixels that are not part of the Face Mask as black\n",
    "pred_mask = cv2.resize(1.0*(model.predict(x=np.array([MNet_scaled]))[0] > 0.5), (img_w,img_h)) # 0.5 is used as the threshold\n",
    "\n",
    "# # commenting below due to error\n",
    "# image_mask = test_img\n",
    "\n",
    "# image_mask[:,:,0] = pred_mask*image[:,:,0]\n",
    "# image_mask[:,:,1] = pred_mask*image[:,:,1]\n",
    "# image_mask[:,:,2] = pred_mask*image[:,:,2]\n",
    "\n",
    "# plt.imshow(image_mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(pred_mask); # show the image with mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets predict the face mask using our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_predict=[]\n",
    "\n",
    "test_img = cv2.resize(p_img, dsize = (img_w, img_h), interpolation = cv2.INTER_CUBIC)\n",
    "test_img = test_img[:, :, :3]\n",
    "test_img = preprocess_input(np.array(test_img, dtype = np.float32))\n",
    "images_to_predict.append(test_img)\n",
    "images_to_predict = np.array(images_to_predict)\n",
    "\n",
    "test_img.shape, images_to_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image to be predicted\n",
    "\n",
    "fig = plt.figure(figsize = (15, 7.2))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(test_img);\n",
    "plt.savefig('image.jpg', bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weight from training and predict on the test image\n",
    "\n",
    "model.load_weights('model_0.29.h5')\n",
    "y_pred = model.predict(np.array(images_to_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the face mask image\n",
    "pred_mask = cv2.resize(1.0*(y_pred[0] > 0.1), (224, 224))\n",
    "\n",
    "image2 = test_img\n",
    "image2[:,:,0] = pred_mask*test_img[:,:,0]\n",
    "image2[:,:,1] = pred_mask*test_img[:,:,1]\n",
    "image2[:,:,2] = pred_mask*test_img[:,:,2]\n",
    "out_image = image2\n",
    "\n",
    "fig = plt.figure(figsize = (15, 7.2))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(out_image)\n",
    "\n",
    "fig = plt.figure(figsize = (15, 7.2))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(pred_mask, alpha = 1)\n",
    "plt.savefig('mask.jpg', bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now show the face mask on the original image\n",
    "\n",
    "img  = cv2.imread('image.jpg', 1)\n",
    "mask = cv2.imread('mask.jpg', 1)\n",
    "img  = cv2.add(img, mask)\n",
    "imposed_img = cv2.addWeighted(src1=img, alpha=1, src2=mask, beta=0.0, gamma=0)\n",
    "\n",
    "fig = plt.figure(figsize = (15, 7.2))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(imposed_img, alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the model\n",
    "model.save(\"Face Detection Model by Aryan Jain.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF PART 1 OF THE PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
